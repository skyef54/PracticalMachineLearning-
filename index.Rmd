---
title: "Prediction Assignment Writeup"
author: "Aleksandr Smetanin"
subtitle: Practical Machine Learning Course Project
date: '25 April 2016'
output: html_document
---

## Introduction
This work is dedicated to qualitative activity recognition. While many articles are concerned with recognising *which* activity is performed, this research is concerned with recognising *how well* it is performed.

Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway
(Class D) and throwing the hips to the front (Class E). Class A corresponds to the correct execution of the exercise, while the other 4 classes correspond to common mistakes. The data was recorded by four 9 degrees of freedom Razor inertial measurement units (IMU), which provide three-axes acceleration, gyroscope and magnetometer data at a joint sampling rate of 45 Hz. The sensors was mounted in the sportsmens' glove, armband, lumbar belt and dumbbell [1].

The goal of the work is to predict Class of movements registered by IMUs.

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. Two data sets were provided: one for [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and one for [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) purposes.

## Data rocessing

### Loading data

The data is supplied in two separate data set. We read training data into *df* data frame and testing data into *mainTest* data frame.

```{r load, echo=FALSE, cache=F}
setwd("~/Coursera/08 Practical Machine Learning/Course project")
# check are there train and test files and download them if necessary
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
fileTrain <- "pml-training.csv"
fileTest <- "pml-testing.csv"
if (!file.exists(fileTrain)) download.file(urlTrain, destfile = fileTrain)
if (!file.exists(fileTest)) download.file(urlTest, destfile = fileTest)

# read data
df <- read.csv(fileTrain)
mainTest <- read.csv(fileTest)
```

The test set will be used only for prediction purpose. There are only 20 cases to predict Class.

```{r}
dim(mainTest)
```

The training set will be used for all other activities.
```{r}
dim(df)
```

There are as many as `r dim(df)[1]` observations. Outcome variable is *classe* that is factor with 5 levels.

```{r}
table(df$classe)
```



### Data preparation

We perform some data preparation steps prior to building our model [2, 3].

We split the *df* set into *training2* and *testing2* sets. This *testing2* set will be used to evaluate performance of a model. We use *createDataPartition* function to create stratified random splits. In this case, 75% of data goes to the new training set and 25% - to the new testing one.

```{r, echo=F, cache=F}
#load libraries
library(caret, verbose = FALSE, warn.conflicts = FALSE, quietly = T)
library(dplyr, warn.conflicts = FALSE, verbose = FALSE)
library(parallel, warn.conflicts = FALSE, verbose = FALSE)
library(doParallel, warn.conflicts = FALSE, verbose = FALSE)

set.seed(1516)
# make training and testing sets
inTrain <- createDataPartition(y = df$classe, p = 0.75, list = FALSE)
training2 <- df[inTrain,]
testing2 <- df[-inTrain,]
dim(training2)
dim(testing2)
```

There are `r dim(training2)[1]` and `r dim(testing2)[1]` observations of `r dim(training2)[2]` variables in the splits.

### Dimension Reduction

We use *nearZeroVar* function to remove so-called "near zero-variance predictors" those can not explain the variation in target variable and can cause problems during resampling for some model types.

```{r, echo=F, cache=F}
# remove near zero variance predictors
nzv <- nearZeroVar(training2)
training2 <- training2[,-nzv]
dim(training2)
```

We eliminate `r length(nzv)` predictors from the data. Also we look are there any missing values?

```{r}
table(summary(training2)[7,])
columnsNA <- which(grepl("NA's", summary(training2)[7,]))
(NAs <- sum(is.na(training2[columnsNA[1]])))
training2 <- training2[,-columnsNA]
dim(training2)
```

Number of predictors with missing values is `r length(columnsNA)`. Each of those predictors have `r NAs` empty values or `r round(NAs/nrow(training2), 2)*100`% of total observations. We drop this variables as they have too little details for our purpose.

We check the remaining predictors for multicollinearity. High correlation can lower down the performance of model. We remove variables with correlation greater than 0.9.

```{r, echo=F, }
# move non-numeric features to the end
numVars <- sapply(1:ncol(training2), FUN=function(x) {is.numeric(training2[,x])})
otherVars <- which(!numVars)
numVars <- which(numVars)
training2 <- select(training2, c(numVars, otherVars))

# remove highly correlated features
corrMatrix <- cor(training2[1:length(numVars)])
highCorr <- findCorrelation(corrMatrix, 0.9)
training2 <- training2[,-highCorr]
dim(training2)
```

At first non-numeric predictors moved to the end of data to drop features correctly. We use *findCorrelation* function to detect and eliminate `r length(highCorr)` highly correlated variables.

Also we drop *X* variables as it is just a row number.
```{r}
# remove X feature as it is row number
training2 <- select(training2, -X)
dim(training2)
```

Dimension reduction steps allow to have only `r ncol(training2)` variables instead of initial number of `r ncol(df)`.

## Model building

## References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. URL http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf

[2] Kuhn, Max. Building Predictive Models in R Using the caret Package. Journal of Statistical Software. November 2008, Volume 28, Issue 5. URL https://www.jstatsoft.org/index.php/jss/article/view/v028i05/v28i05.pdf

[3] SUNIL RAY. Beginners Guide To Learn Dimension Reduction Techniques. JULY 28, 2015. URL http://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/


